{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9de16e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:17:20.735838Z",
     "start_time": "2021-12-08T21:17:20.697938Z"
    },
    "code_folding": [
     7,
     36,
     58,
     102,
     146,
     167,
     215,
     254,
     322,
     336,
     363,
     414,
     435,
     441,
     449,
     455,
     462,
     468,
     480,
     490,
     513,
     519
    ]
   },
   "outputs": [],
   "source": [
    "# By Ken, date: 2021.12.8\n",
    "import numpy as np # version: 1.20.1\n",
    "import pandas as pd # version: 1.2.4\n",
    "import matplotlib.pyplot as plt # version: 3.3.4\n",
    "from sklearn.preprocessing import StandardScaler  # sklearn version: 0.24.1\n",
    "\n",
    "class FNN():\n",
    "    '''\n",
    "    this is a implementation of a fully connected neural network (FNN) \n",
    "    using Stochastic Gradient Descent (SGD) and backprop algorithm.\n",
    "    hidden layer activation currently support only ReLu.\n",
    "    output layer activation currently support only sigmoid.\n",
    "    \n",
    "    Interface:\n",
    "        add_layer(), add_output_layer() method can be used to construction the architecture of the neural network.\n",
    "        Input() method can be used to input training data.\n",
    "        train() method is used to start the training process.\n",
    "        predict() method is used to calculate prediction.\n",
    "        export_model() method can be used to export parameters of a trained network for later model reconstruction.\n",
    "        load_model() method can be used to reconstruct of model.\n",
    "        \n",
    "    user can also play around with other internal use method, such as:\n",
    "        forward_prop() : forward pass algorithm\n",
    "        backward_prop() : backward propogation algorithm\n",
    "    '''\n",
    "    def __init__(self, epochs=10, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.w = {} # weights of the network, dictionary key is be the name of the layer\n",
    "        self.layer_name_list = [] # store layer names for each layer in order\n",
    "        self.output_layer_flag = 0 # indicate if the network contains output layer or not\n",
    "        self.xtrain = np.array([]) # store training data\n",
    "        self.ytrain = np.array([]) # store training data\n",
    "        self.history_training_cost = None # store cost for each epochs during trianing\n",
    "        \n",
    "    def Input(self, xtrain, ytrain):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        feed in training data (n_samples, n_features) and label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        xtrain : ndarray, DataFrame\n",
    "            training data X, will be converted to ndarray, and added bias column.\n",
    "\n",
    "        ytrain : ndarray, arraylike\n",
    "            label data y, will be converted to ndarray.\n",
    "        '''\n",
    "        # convert to numpy array if it is a pandas dataframe\n",
    "        if not isinstance(xtrain, np.ndarray): \n",
    "            xtrain = xtrain.values\n",
    "            ytrain = ytrain.values\n",
    "        self.xtrain = self.augmentation(xtrain) # add bias column\n",
    "        self.ytrain = ytrain\n",
    "        return self\n",
    "        \n",
    "    def add_layer(self, n_inputs, n_units, layer_name, random_state=None):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        adding a hidden layer on top of the existing nueral network architecture, output layer should yet be defined.\n",
    "        'He initialization' is used for weight initialization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            number of inputs of this layer.\n",
    "\n",
    "        n_units : int\n",
    "            number of neurons (outputs) of this layer.\n",
    "            \n",
    "        layer_name : str\n",
    "            unique name to identify this layer.\n",
    "            \n",
    "        random_state : int, default None\n",
    "            random seed to control weight initialization.\n",
    "        '''\n",
    "        # if output layer is defined, cannot add layer further\n",
    "        if self.output_layer_flag == 1:\n",
    "            print(\"add layer fail: output layer already defined, cannot add another layer!\")\n",
    "            return self\n",
    "        \n",
    "        # check if number of input is consistent with that of previous layer\n",
    "        if len(self.layer_name_list):      \n",
    "            previous_layer_name = self.layer_name_list[-1]\n",
    "            previous_layer = self.w[previous_layer_name]\n",
    "            if n_inputs != previous_layer.shape[0]:\n",
    "                print('*** add layer fail: n_inputs is inconsistent with the previous layer output shape!')\n",
    "                return self\n",
    "        \n",
    "        # layer_name need to be unique for each layer in the structure\n",
    "        if len(self.layer_name_list) and (layer_name in self.layer_name_list):\n",
    "            print('add layer fail: layer name already used')\n",
    "            return self\n",
    "        \n",
    "        # He initialization technique is used to generate random weights for this layer\n",
    "        self.w[layer_name] = self.He_initialization(n_units, n_inputs, random_state=random_state) # store this weights\n",
    "        self.layer_name_list.append(layer_name) # update the layer name list for the network\n",
    "        return self\n",
    "    \n",
    "    def add_output_layer(self, n_outputs, layer_name, random_state=None):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        adding a output layer on top of the existing network architecture, should have at least one hidden layer in the nueral network.\n",
    "        number of inputs of this layer is inferred from previous layer.\n",
    "        'He initialization' is used for weight initialization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_outputs : int\n",
    "            number of neurons (outputs) of this layer.\n",
    "            \n",
    "        layer_name : str\n",
    "            unique name to identify this layer.\n",
    "            \n",
    "        random_state : int, default None\n",
    "            random seed to control weight initialization.\n",
    "        '''\n",
    "        # check if output layer already exists\n",
    "        if self.output_layer_flag == 1:\n",
    "            print(\"add output layer fail: output layer already defined!\")\n",
    "            return self\n",
    "        \n",
    "        # check at least having 1 hidden layer\n",
    "        if len(self.layer_name_list)==0:           \n",
    "            print(\"add output layer fail: 0 depth network structure!\")\n",
    "            return self\n",
    "        \n",
    "        # check layer name is unique\n",
    "        if len(self.layer_name_list) and (layer_name in self.layer_name_list):\n",
    "            print('add output layer fail: layer name already used')\n",
    "            return self\n",
    "        \n",
    "        # infer number of inputs of this layer from the previous layer\n",
    "        previous_layer_name = self.layer_name_list[-1]\n",
    "        n_inputs = self.w[previous_layer_name].shape[0]\n",
    "        \n",
    "        # He initialization technique is used to generate random weights for this layer\n",
    "        self.w[layer_name] = self.He_initialization(n_outputs, n_inputs, random_state=random_state) # store this weights\n",
    "        self.layer_name_list.append(layer_name) # update the layer name list for the network\n",
    "        self.output_layer_flag = 1 # flag: output layer is defined\n",
    "        return self\n",
    "        \n",
    "    def train(self,):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        use Stochastic Gradient Descent (SGD) to train the nueral network\n",
    "        '''\n",
    "        # check network at least has two layer, 1 hidden layer, 1 output layer\n",
    "        if len(self.w)<2:\n",
    "            print('train fail: network depth need to be larger than 1, and including 1 output layer')\n",
    "            return\n",
    "        \n",
    "        # check if output layer is already defined\n",
    "        if self.output_layer_flag==0:\n",
    "            print('train fail: missing output layer!')\n",
    "            return\n",
    "        \n",
    "        # stochastic gradient descend\n",
    "        self.SGD(self.epochs)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def SGD(self, epochs):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        perform Stochastic Gradient Descent (SGD)\n",
    "        calculate gradient using one randomly selected sample from X at each iteration\n",
    "        loss function: cross entropy loss\n",
    "        use forward propogation to calculate outputs\n",
    "        use backward propogation to update weights\n",
    "        \n",
    "        training cost for each epochs is store in: self.history_training_cost\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : int\n",
    "            number of epochs.\n",
    "        '''\n",
    "        n_samples = self.xtrain.shape[0] # number of samples in training dataset\n",
    "        index_arr = np.arange(n_samples) # array of index to perform stochastic selection\n",
    "        history_training_cost = [] # store cost for each epochs\n",
    "        for epoch in range(epochs):\n",
    "            # for each epochs: shuffle index to simulate random selection of samples to calculate sgd\n",
    "            np.random.shuffle(index_arr)\n",
    "            for i in index_arr:\n",
    "                sample = self.xtrain[i,:].reshape((-1,1)) # convert this stochastically selected sample to column vector\n",
    "                label = self.ytrain[i]\n",
    "                \n",
    "                # forward pass to calculate output at each node (neuron unit) for the current iteration\n",
    "                # notation: z = W*x, h = g(z), where g(x) is the activation function like ReLu or tanh\n",
    "                z_dict, h_dict = self.forward_prop(sample) # all result store in dictionary format, keys are layer name\n",
    "                \n",
    "                # backward propogation to calculate gradient of W \n",
    "                gradient_w, _ = self.backward_prop(z_dict, h_dict, sample, label) # all result store in dictionary format, keys are layer name\n",
    "                \n",
    "                # update W for each layer based on gradient \n",
    "                for layer_name in self.layer_name_list:\n",
    "                    self.w[layer_name] = self.w[layer_name] - self.learning_rate * gradient_w[layer_name]\n",
    "            \n",
    "            # calculate cost of training data after each epoch\n",
    "            train_yhat = self.predict(self.xtrain, threshold=0.5, add_bias=False, return_proba=True) # use this newly updated weights to predict the label: yhat\n",
    "            train_loss = self.cross_entropy_loss(self.ytrain, train_yhat) # calculate cross entropy loss for each sample in training set\n",
    "            train_cost = train_loss.sum() / len(self.ytrain) # cost = average cross entropy\n",
    "            history_training_cost.append(train_cost) # store result\n",
    "\n",
    "        # save result\n",
    "        self.history_training_cost = history_training_cost \n",
    "        return self\n",
    "                \n",
    "    def forward_prop(self, sample):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        perform forward propogation to calculate output at each layer\n",
    "        activation for hidden layers is ReLu\n",
    "        activation for output layer is sigmoid\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : ndarray\n",
    "            column vector of sample.\n",
    "            \n",
    "        Return\n",
    "        ----------\n",
    "        z_dict : dict\n",
    "            result of z (value before apply activation) for each layer\n",
    "        h_dict : dict\n",
    "            result of h (output of the layer) for each layer\n",
    "        '''\n",
    "        # using ReLu for activation in hidden layer\n",
    "        cur_input = sample # input (column vector) of current layer\n",
    "        z_dict = {}# store output of each layer in this iteration\n",
    "        h_dict = {} # store z = W*x of each layer in this iteration\n",
    "        for i in self.layer_name_list:\n",
    "            cur_w = self.w[i] # weights of current layer\n",
    "            cur_z = np.dot(cur_w, cur_input) # W*x\n",
    "            z_dict[i] = cur_z # store result\n",
    "            \n",
    "            if i!=self.layer_name_list[-1]: # if not output layer, use activation ReLu\n",
    "                cur_h = self.ReLu(cur_z) # output of current layer: h = ReLu(W*x)\n",
    "            else: # output layer use sigmoid activation\n",
    "                cur_h = self.sigmoid(cur_z)\n",
    "            \n",
    "            h_dict[i] = cur_h # store result\n",
    "                \n",
    "            cur_input = np.insert(cur_h, 0, 1, axis=0) # input of next layer is output of this layer, insert bias term\n",
    "        return z_dict, h_dict # z, h are column vectors, like sample\n",
    "        \n",
    "    def backward_prop(self, z_dict, h_dict, sample, label):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        perform backward propogation, calculate gradient \n",
    "        activation for hidden layers is ReLu\n",
    "        activation for output layer is sigmoid\n",
    "        loss function: cross entropy loss\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        z_dict : dict\n",
    "            result z from forward propogation.\n",
    "        h_dict : dict\n",
    "            result h from forward propogation.\n",
    "        sample : ndarray\n",
    "            column vector of sample.\n",
    "        label : int\n",
    "            label of the sample.\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        gradient_w : dict\n",
    "            gradient of w at this iteration\n",
    "        gradient_z : dict\n",
    "            gradient of z at this iteration\n",
    "        '''\n",
    "        gradient_w = {i:None for i in self.layer_name_list} # store gradient of w for each layer\n",
    "        gradient_z = {i:None for i in self.layer_name_list} # store gradient of z for each layer\n",
    "        n = len(self.layer_name_list)\n",
    "        for i in range(n-1,-1,-1):# starting from last layer\n",
    "            cur_layer_name = self.layer_name_list[i]\n",
    "            if i==n-1: # this is output layer use subscript d to represent\n",
    "                # 1. dJ/dz_d = -t + sigmoid(z_d), for cross entropy loss gradient, sigmoid activation \n",
    "                dJdz = -label + self.sigmoid(z_dict[cur_layer_name]) # dJ/dz_d , column vector\n",
    "                gradient_z[cur_layer_name] = dJdz # store result\n",
    "                \n",
    "                # 2. ▽(w_d)J = dJ/dz_d * (1 (h_d-1)^T)\n",
    "                previous_layer_name = self.layer_name_list[i-1]\n",
    "                A = np.insert(h_dict[previous_layer_name], 0, 1, axis=0).T # (1 (h_d-1)^T)\n",
    "                gradient_w[cur_layer_name] = np.dot(dJdz, A) # ▽(w_d)J, store gradient for output layer\n",
    "                \n",
    "                # 3. ▽(z_d-1)J = g'(z_d-1) *. ((W_bar_d)^T * dJ/dz_d), g' is derivative of ReLu, either 0 or 1\n",
    "                B = z_dict[previous_layer_name] # z_d-1\n",
    "                C = self.derivative_ReLu(B) # g'(z_d-1)\n",
    "                D = self.w[cur_layer_name][:,1:].T # (W_bar_d)^T, note: W_bar means does not contain bias term\n",
    "                gradient_z[previous_layer_name] = C * np.dot(D, dJdz) # ▽(z_d-1)J , store result\n",
    "            \n",
    "            elif i==0: # this is the first layer\n",
    "                # 1. ▽(w_1)J = ▽(z_1)J * (1 x^T)\n",
    "                A = gradient_z[cur_layer_name] # ▽(z_1)J\n",
    "                B = sample.T # (1 x^T)\n",
    "                gradient_w[cur_layer_name] = np.dot(A, B) # ▽(w_1)J , store result\n",
    "                \n",
    "            else: # this is a hidden layer\n",
    "                previous_layer_name = self.layer_name_list[i-1]\n",
    "                # 1. ▽(w_j)J = ▽(z_j)J * (1 (h_j-1)^T)\n",
    "                A = gradient_z[cur_layer_name] # ▽(z_j)J\n",
    "                B = np.insert(h_dict[previous_layer_name], 0, 1, axis=0).T # (1 (h_j-1)^T)\n",
    "                gradient_w[cur_layer_name] = np.dot(A, B)\n",
    "                # 2. ▽(z_j-1)J = g'(z_j-1) *. ((W_bar_j)^T * ▽(z_j)J)\n",
    "                C = z_dict[previous_layer_name] # z_j-1\n",
    "                D = self.derivative_ReLu(C) # g'(z_j-1)\n",
    "                E = self.w[cur_layer_name][:,1:].T # (W_bar_j)^T, note: W_bar means does not contain bias term\n",
    "                gradient_z[previous_layer_name] = D * np.dot(E, A) # ▽(z_d-1)J , store result\n",
    "        \n",
    "        return gradient_w, gradient_z\n",
    "        \n",
    "    def augmentation(self, x):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        return a copy of x with added bias column\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "        '''\n",
    "        n, m = x.shape\n",
    "        ones = np.ones((n, 1))\n",
    "        return np.hstack((ones, x))\n",
    "            \n",
    "    def He_initialization(self, n_units, n_inputs, random_state=None):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        perform weight initialization for a layer\n",
    "        He Initialization is a popular choice for ReLu\n",
    "        defined as: zero-mean Gaussian distribution whose standard deviation (std) is sqrt(2/n), \n",
    "        where n is the number of input feature of the layer\n",
    "        initialize b (bias column) = 0\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_units : int\n",
    "            number of neurons (outputs) of the layer.\n",
    "            \n",
    "        n_inputs : int\n",
    "            number of inputs of the layer.\n",
    "            \n",
    "        random_state : int, default None\n",
    "            random seed to control weight initialization.\n",
    "        '''\n",
    "        rng = np.random.default_rng(random_state) # random generator\n",
    "        std = np.sqrt(2/n_inputs) # standard deviation (std) is sqrt(2/n)\n",
    "        init_w = rng.normal(loc=0, scale=std, size=(n_units, n_inputs)) # 0 mean, std=sqrt(2/n) gaussian distribution\n",
    "        b = np.zeros((n_units, 1)) # initialize b = 0\n",
    "        return np.hstack((b, init_w))\n",
    "        \n",
    "    def predict(self, x, threshold=0.5, add_bias=True, return_proba=False):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        perform prediction using x and the current weights of the neural network\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            input data of x\n",
    "        \n",
    "        threshold : float, default 0.5\n",
    "            if predicted probabilistic value >= threshold then predicted label is 1, otherwise 0\n",
    "            changing this number to specify weight of label\n",
    "        \n",
    "        add_bias : bool, default True\n",
    "            default is to add bias to x, if x already contains bias column, set this to False.\n",
    "            \n",
    "        return_proba : bool, default False\n",
    "            default is to return predicted label. If True, return predicted probabilistic values.\n",
    "            \n",
    "        Return\n",
    "        ----------\n",
    "        return ndarray of pridected probabilistic values or label depending on the parameter 'return_proba'\n",
    "        '''\n",
    "        # convert to ndarray\n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.values\n",
    "        \n",
    "        # add bias column to x\n",
    "        if add_bias:\n",
    "            x = self.augmentation(x)\n",
    "            \n",
    "        # check consistency of data shape\n",
    "        first_layer = self.layer_name_list[0]\n",
    "        if x.shape[1]!=self.w[first_layer].shape[1]:\n",
    "            print('input shape is inconsistent, number of features should be %d (%d instead)'%(self.w[first_layer].shape[1]-1, x.shape[1]-1))\n",
    "            return None\n",
    "            \n",
    "        # compute the output using w\n",
    "        yhat_list = []\n",
    "        for i in range(len(x)):\n",
    "            sample = x[i,:].reshape((-1,1)) \n",
    "            _, h_dict = self.forward_prop(sample)\n",
    "            yhat = h_dict[self.layer_name_list[-1]][0,0]\n",
    "            yhat_list.append(yhat)\n",
    "        if return_proba:\n",
    "            return np.array(yhat_list) # return probability of label\n",
    "        else:\n",
    "            return (np.array(yhat_list)>=threshold) * 1 # convert to label of 0 and 1\n",
    "    \n",
    "    def cross_entropy_loss(self, y, yhat):\n",
    "        '''\n",
    "        Description\n",
    "        ----------\n",
    "        calculate cross entropy loss : -y*log(yhat) - (1-y)*log(1-yhat)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray\n",
    "            true label\n",
    "        \n",
    "        yhat : ndarray\n",
    "            predicted probabilistic values\n",
    "            \n",
    "        Return\n",
    "        ----------\n",
    "        cross entropy loss\n",
    "        '''\n",
    "        loss = -y*np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "        return loss\n",
    "    \n",
    "    def misclassification(self, y, yhat):\n",
    "        '''\n",
    "        calculate misclassification rate\n",
    "        '''\n",
    "        return (y != yhat).sum() / len(y)\n",
    "    \n",
    "    def ReLu(self, x):\n",
    "        '''\n",
    "        calculate ReLu function\n",
    "        '''\n",
    "        x = x.copy()\n",
    "        x[x<0] = 0\n",
    "        return x\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        calculate sigmoid function\n",
    "        '''\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def derivative_ReLu(self, x):\n",
    "        '''\n",
    "        calculate derivative of ReLu function\n",
    "        '''\n",
    "        x = x.copy()\n",
    "        return (x>0) * 1\n",
    "    \n",
    "    def derivative_sigmoid(self, x):\n",
    "        '''\n",
    "        calculate derivative of sigmoid function\n",
    "        '''\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def export_model(self,):\n",
    "        '''\n",
    "        use this function to export parameters of the model for later reconstruction of this model\n",
    "        '''\n",
    "        d = {\n",
    "            'epochs' : self.epochs,\n",
    "            'learning_rate' : self.learning_rate,\n",
    "            'w' : self.w,\n",
    "            'layer_name_list' : self.layer_name_list,\n",
    "        }\n",
    "        return d\n",
    "    \n",
    "    def load_model(self, d):\n",
    "        '''\n",
    "        accept dictionary of parameters to reconstruction of model\n",
    "        '''\n",
    "        self.epochs = d['epochs']\n",
    "        self.learning_rate = d['learning_rate']\n",
    "        self.w = d['w']\n",
    "        self.layer_name_list = d['layer_name_list']\n",
    "        return self\n",
    "\n",
    "def split(data, train_ratio=0.7, val_ratio=0.15, random_state=None):\n",
    "    '''\n",
    "    split data into training, validation, testing set, the results maintain the same distribution \n",
    "    '''\n",
    "    # shuffle data before spliting\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    data = rng.permutation(data)\n",
    "\n",
    "    data_1 = data[data[:,-1]==1] # seperate label 1\n",
    "    data_2 = data[data[:,-1]==0] # seperate label 0\n",
    "    # split train, val, test set\n",
    "    n1 = data_1.shape[0]\n",
    "    n2 = data_2.shape[0]\n",
    "    train_split_1 = int(n1*train_ratio)\n",
    "    train_split_2 = int(n2*train_ratio)\n",
    "    val_split_1 = int(n1*(train_ratio+val_ratio))\n",
    "    val_split_2 = int(n2*(train_ratio+val_ratio))\n",
    "    \n",
    "    train = np.vstack((data_1[:train_split_1], data_2[:train_split_2]))\n",
    "    val = np.vstack((data_1[train_split_1:val_split_1], data_2[train_split_2:val_split_2]))\n",
    "    test = np.vstack((data_1[val_split_1:], data_2[val_split_2:]))\n",
    "    return train, val, test\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    '''\n",
    "    calculate accuracy\n",
    "    '''\n",
    "    return (y == yhat).sum() / len(y)\n",
    "\n",
    "def misclassification(y, yhat):\n",
    "    '''\n",
    "    calculate misclassification\n",
    "    '''\n",
    "    return (y != yhat).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1416d4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:17:21.440919Z",
     "start_time": "2021-12-08T21:17:21.429947Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# --------------- data preprocessing ---------------\n",
    "# load data\n",
    "df = pd.read_csv('../data_banknote_authentication.txt', header=None)\n",
    "data = df.values\n",
    "# split data\n",
    "train, val, test = split(data, train_ratio=0.7, val_ratio=0.15, random_state=1)\n",
    "x_train, y_train = train[:,:-1], train[:,-1]\n",
    "x_val, y_val = val[:,:-1], val[:,-1]\n",
    "x_test, y_test = test[:,:-1], test[:,-1]\n",
    "# standardize data\n",
    "std = StandardScaler().fit(x_train)\n",
    "x_train = std.transform(x_train)\n",
    "x_val = std.transform(x_val)\n",
    "x_test = std.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7612eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:17:40.524516Z",
     "start_time": "2021-12-08T21:17:36.440986Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model...\n",
      "training done!\n"
     ]
    }
   ],
   "source": [
    "# --------------- build binary classification network structure ---------------\n",
    "# netwrok structure:\n",
    "#  - first hidden layer: n1 units\n",
    "#  - second hidden layer: n2 units\n",
    "#  - output layer: 1 units\n",
    "n1 = 2\n",
    "n2 = 2\n",
    "epochs = 20\n",
    "learning_rate=0.01\n",
    "print('training model...')\n",
    "fnn = FNN(epochs=epochs, learning_rate=learning_rate) # initialize network\n",
    "fnn = fnn.Input(x_train, y_train) # input the training dataset and validation dataset\n",
    "fnn = fnn.add_layer(4, n1, layer_name='1') # hidden layer1: 2 nueron\n",
    "fnn = fnn.add_layer(n1, n2, layer_name='2') # hidden layer2: 2 nueron\n",
    "fnn = fnn.add_output_layer(1, layer_name='output') # output layer with 1 output unit\n",
    "fnn = fnn.train() # train the network\n",
    "print('training done!')\n",
    "# store result\n",
    "train_cost = fnn.history_training_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2d17023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:17:43.813275Z",
     "start_time": "2021-12-08T21:17:43.643572Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2277c2a5430>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------- plot learning curve: training cost at each epoch ---------------\n",
    "epochs = range(1,len(train_cost)+1)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(epochs, train_cost, label='cross entropy loss')\n",
    "plt.title('learning curve', size=15)\n",
    "plt.xlabel('epochs',size=12)\n",
    "plt.ylabel('cross entropy loss',size=12)\n",
    "plt.xticks(size=12)\n",
    "plt.yticks(size=12)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f59683dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:17:49.430471Z",
     "start_time": "2021-12-08T21:17:49.335227Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set prediction accuracy =  0.9927083333333333\n",
      "test set prediction accuracy =  0.9951690821256038\n"
     ]
    }
   ],
   "source": [
    "# --------------- prediction ---------------\n",
    "yhat_train = fnn.predict(x_train)\n",
    "print('training set prediction accuracy = ', accuracy(y_train, yhat_train))\n",
    "\n",
    "yhat_test = fnn.predict(x_test)\n",
    "print('test set prediction accuracy = ', accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0371539a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:17:55.443158Z",
     "start_time": "2021-12-08T21:17:55.427163Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': array([[ 0.46673763, -2.19224602, -1.69881321, -1.75062465, -0.03518804],\n",
       "        [ 0.63121014, -0.18003656,  1.48068689,  0.68165308,  0.136889  ]]),\n",
       " '2': array([[-0.33856153,  2.17269037,  0.28668158],\n",
       "        [ 1.6857825 , -0.83782329,  2.04833327]]),\n",
       " 'output': array([[-1.14893215,  1.73241182, -2.64466075]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to access the weight of the model\n",
    "fnn.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef5ba181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T21:18:22.275966Z",
     "start_time": "2021-12-08T21:18:22.256019Z"
    },
    "code_folding": [
     7,
     36,
     58,
     102,
     146,
     167,
     215,
     254,
     322,
     336,
     363,
     414,
     435,
     441,
     449,
     455,
     462,
     468,
     480,
     490,
     513,
     519
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set prediction accuracy of reconstructed model =  0.9951690821256038\n"
     ]
    }
   ],
   "source": [
    "# after getting a satisfying model, we can save the paramters of the model for later use\n",
    "model_parameters_dict = fnn.export_model()\n",
    "# load model with this weight\n",
    "my_model = FNN().load_model(model_parameters_dict)\n",
    "# the model can now be use to do prediction\n",
    "yhat_test = my_model.predict(x_test)\n",
    "print('test set prediction accuracy of reconstructed model = ', accuracy(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c53b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32277c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
