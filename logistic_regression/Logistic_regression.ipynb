{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac97d494",
   "metadata": {},
   "source": [
    "## Note\n",
    "### 1. 线性回归模型\n",
    "- x：特征矩阵\n",
    "- w：权值向量，同线性模型的参数向量\n",
    "- b：截距\n",
    "$$y=x\\cdot w+b$$\n",
    "\n",
    "### 2. 二分类逻辑回归模型\n",
    "- Y：标签\n",
    "- 左侧为Y=1的条件概率\n",
    "$$P(Y=1|x)=\\frac{1}{1+e^{-(x\\cdot w+b)}}$$\n",
    "\n",
    "### 3. 极大似然与损失函数\n",
    "- 设：\n",
    "$$P(Y=1|x)=p(x)$$\n",
    "$$P(Y=0|x)=1-p(x)$$\n",
    "- 通过极大似然使得概率值逼近标签值\n",
    "    - xi:一个样本\n",
    "    - yi：i样本对应的真实标签\n",
    "$$MLE=ArgMax \\prod [p(x_{i})]^{y_{i}}*[1-p(x_{i})]^{1-y_{i}}$$\n",
    "- 损失函数 logloss\n",
    "    - n：样本数\n",
    "$$LogLoss = ArgMin-\\frac{1}{n}(\\sum_{i=1}^{n}(y_{i}*ln(p(x_{i}))+(1-y_{i})*ln(1-p(x_{i})))$$\n",
    "\n",
    "### 4. 梯度下降\n",
    "$$g_{i} = \\frac{\\partial LogLoss}{\\partial w}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}*(p(x_{i})-y_{i})$$\n",
    "\n",
    "- 通过迭代，更新w得到最优损失函数\n",
    "    - j：第j轮迭代\n",
    "    - lambda：学习率\n",
    "$$w_{i}^{j+1}=w_{i}^{j}-\\lambda g_{i}$$\n",
    "\n",
    "### 5. 逻辑回归最终预测模型\n",
    "$$LogisticModel=\\left \\{ \n",
    "\\begin{aligned}\n",
    "1 \\quad \\frac{1}{1+e^{-(x\\cdot w+b)}}>0.5 \\\\\n",
    "-1  \\quad \\frac{1}{1+e^{-(x\\cdot w+b)}}\\leq 0.5\n",
    "\\end{aligned}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b8824ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:34:53.489139Z",
     "start_time": "2021-12-09T04:34:53.479154Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # version: 1.20.1\n",
    "import matplotlib.pyplot as plt # version: 3.3.4\n",
    "from sklearn.datasets import load_breast_cancer # sklearn version: 0.24.1\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c72cb71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:38:07.961164Z",
     "start_time": "2021-12-09T04:38:07.937762Z"
    },
    "code_folding": [
     80,
     86,
     96,
     99,
     104,
     109
    ]
   },
   "outputs": [],
   "source": [
    "class logistic_regression():\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, batch_size=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter # maximum number of iterations\n",
    "        self.batch_size = batch_size # batch size of the gradient descent. If 1, it is SGD\n",
    "        self.w_history = [] # store weights at each iteration during the training\n",
    "        self.cost_history = [] # store cost at each iteration during the training\n",
    "    \n",
    "    def fit(self, xtrain, ytrain):\n",
    "        # train the model with the input training data, accept ndarray\n",
    "        xtrain = self.augmentation(xtrain) # add bias column to X\n",
    "        \n",
    "        # batch gradient descent\n",
    "        self.w_history, self.cost_history = self.gredient_descent(xtrain, ytrain)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def gredient_descent(self, x, t):\n",
    "        # x is training data, t is target\n",
    "        n, m = x.shape\n",
    "        w = np.zeros(m) # initialize w\n",
    "        \n",
    "        w_history = [] # store history weights in the processing of training\n",
    "        cost_history = []\n",
    "        for i in range(self.max_iter):\n",
    "\n",
    "            sample_index = np.random.randint(0, n, self.batch_size) # stochastic selection of samples\n",
    "            xx = x[sample_index]\n",
    "            tt = t[sample_index]\n",
    "\n",
    "            z = np.dot(w, xx.T) # calculate X*w, shape=(n,1)\n",
    "            y = self.sigmoid(z) # calculate sigmoid(X*w), shape=(n,1)\n",
    "\n",
    "            gd = 1/n * np.dot(xx.T, (y - tt)) # calculate gredient\n",
    "\n",
    "            w = w - self.learning_rate * gd # update w\n",
    "\n",
    "            w_history.append(w) # record each w\n",
    "\n",
    "            self.w = w # update w\n",
    "            \n",
    "            # based on the updated weights compute cost for the training data: log loss or cross entropy loss\n",
    "            z = np.dot(self.w, x.T)\n",
    "            y = self.sigmoid(z)\n",
    "            train_loss = self.cross_entropy_loss(t, y)\n",
    "            train_cost = train_loss.sum() / len(y)\n",
    "            \n",
    "            cost_history.append(train_cost)\n",
    "        \n",
    "        return w_history, cost_history\n",
    "    \n",
    "    def predict(self, x, threshold=0.5, add_bias=True, return_proba=False):\n",
    "        # convert to ndarray\n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.values\n",
    "        # add bias column to x\n",
    "        if add_bias:\n",
    "            x = self.augmentation(x)\n",
    "        z = np.dot(x, self.w)\n",
    "        yhat = self.sigmoid(z).ravel()\n",
    "        if return_proba:\n",
    "            return yhat\n",
    "        else:\n",
    "            return (yhat >= threshold) * 1\n",
    "        \n",
    "    def augmentation(self, x):\n",
    "        # add bias column to x\n",
    "        n, m = x.shape\n",
    "        ones = np.ones((n, 1))\n",
    "        return np.hstack((ones, x))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # sigmoid function\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def cross_entropy_loss(self, y, yhat):\n",
    "        # calculate cross entropy loss : -y*log(yhat) - (1-y)*log(1-yhat)\n",
    "\n",
    "        loss = -y*np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "        return loss\n",
    "\n",
    "def split_data(data, target, ratio=0.8):\n",
    "    n = data.shape[0]\n",
    "    ind = np.arange(n)\n",
    "    split = int(n*ratio)\n",
    "    return data[:split], target[:split], data[split:], target[split:]\n",
    "\n",
    "def confusion_matrix(y, y_pred):\n",
    "    # binary classification confusion matrix\n",
    "    ind_t = np.nonzero(y==1)[0] # positive label index\n",
    "    ind_f = np.nonzero(y==0)[0] # negative label index\n",
    "    tp = (y_pred[ind_t]==1).sum() # true positive\n",
    "    fn = (y_pred[ind_t]==0).sum() # false negative\n",
    "    tn = (y_pred[ind_f]==0).sum() # true negative\n",
    "    fp = (y_pred[ind_f]==1).sum() # false negative\n",
    "    return tp,fn,tn,fp\n",
    "\n",
    "def misclassification_rate(y, y_pred):\n",
    "    return (y!=y_pred).sum() / len(y)\n",
    "\n",
    "def precision(y, y_pred):\n",
    "    # precision = TP / (TP+FP)  \n",
    "    tp,fn,tn,fp = confusion_matrix(y, y_pred)\n",
    "    return tp / (tp+fp)\n",
    "\n",
    "def recall(y, y_pred):\n",
    "    # recall = TP / (TP+FN)\n",
    "    tp,fn,tn,fp = confusion_matrix(y, y_pred)\n",
    "    return tp / (tp+fn)\n",
    "\n",
    "def f1_score(y, y_pred):\n",
    "    # calculate f1 score = 2* (precision*recall) / (precision+recall)\n",
    "    p = precision(y, y_pred)\n",
    "    r = recall(y, y_pred)\n",
    "    return 2 * (p*r) / (p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a80655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:38:08.162677Z",
     "start_time": "2021-12-09T04:38:08.136785Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_breast_cancer()\n",
    "data = iris.data # (569, 30)\n",
    "target = iris.target # (569,), label:0,1\n",
    "\n",
    "# split data\n",
    "xtrain, ytrain, xtest, ytest = split_data(data, target, 0.8)\n",
    "\n",
    "# standardization\n",
    "std = StandardScaler().fit(xtrain)\n",
    "std_xtrain = std.transform(xtrain)\n",
    "std_xtest = std.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bcdf131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:41:30.009094Z",
     "start_time": "2021-12-09T04:41:29.973159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of testing set = 0.9714285714285714\n",
      "misclassification rate = 0.043859649122807015\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "batch_size = len(std_xtrain) # using full batch\n",
    "my_lr = logistic_regression(learning_rate=0.01, max_iter=100, batch_size=batch_size)\n",
    "my_lr = my_lr.fit(std_xtrain, ytrain)\n",
    "\n",
    "# predict testing set using different metrics\n",
    "yhat = my_lr.predict(std_xtest)\n",
    "print('f1 score of testing set =', f1_score(ytest, yhat))\n",
    "print('misclassification rate =', misclassification_rate(ytest, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690cdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
