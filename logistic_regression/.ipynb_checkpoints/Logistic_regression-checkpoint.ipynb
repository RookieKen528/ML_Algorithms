{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:54:54.334606Z",
     "start_time": "2021-12-09T04:54:52.904642Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-15T01:14:43.577230Z",
     "iopub.status.busy": "2020-10-15T01:14:43.576228Z",
     "iopub.status.idle": "2020-10-15T01:14:44.991719Z",
     "shell.execute_reply": "2020-10-15T01:14:44.989944Z",
     "shell.execute_reply.started": "2020-10-15T01:14:43.577230Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # version: 1.20.1\n",
    "import matplotlib.pyplot as plt # version: 3.3.4\n",
    "from sklearn.datasets import load_breast_cancer # sklearn version: 0.24.1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class logistic_regression():\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, batch_size=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter # maximum number of iterations\n",
    "        self.batch_size = batch_size # batch size of the gradient descent. If 1, it is SGD\n",
    "        self.w_history = [] # store weights at each iteration during the training\n",
    "        self.cost_history = [] # store cost at each iteration during the training\n",
    "    \n",
    "    def fit(self, xtrain, ytrain):\n",
    "        # train the model with the input training data, accept ndarray\n",
    "        xtrain = self.augmentation(xtrain) # add bias column to X\n",
    "        \n",
    "        # batch gradient descent\n",
    "        self.w_history, self.cost_history = self.gredient_descent(xtrain, ytrain)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def gredient_descent(self, x, t):\n",
    "        # x is training data, t is target\n",
    "        n, m = x.shape\n",
    "        w = np.zeros(m) # initialize w\n",
    "        \n",
    "        w_history = [] # store history weights in the processing of training\n",
    "        cost_history = []\n",
    "        for i in range(self.max_iter):\n",
    "\n",
    "            sample_index = np.random.randint(0, n, self.batch_size) # stochastic selection of samples\n",
    "            xx = x[sample_index]\n",
    "            tt = t[sample_index]\n",
    "\n",
    "            z = np.dot(w, xx.T) # calculate X*w, shape=(n,1)\n",
    "            y = self.sigmoid(z) # calculate sigmoid(X*w), shape=(n,1)\n",
    "\n",
    "            gd = 1/n * np.dot(xx.T, (y - tt)) # calculate gredient\n",
    "\n",
    "            w = w - self.learning_rate * gd # update w\n",
    "\n",
    "            w_history.append(w) # record each w\n",
    "\n",
    "            self.w = w # update w\n",
    "            \n",
    "            # based on the updated weights compute cost for the training data: log loss or cross entropy loss\n",
    "            z = np.dot(self.w, x.T)\n",
    "            y = self.sigmoid(z)\n",
    "            train_loss = self.cross_entropy_loss(t, y)\n",
    "            train_cost = train_loss.sum() / len(y)\n",
    "            \n",
    "            cost_history.append(train_cost)\n",
    "        \n",
    "        return w_history, cost_history\n",
    "    \n",
    "    def predict(self, x, threshold=0.5, add_bias=True, return_proba=False):\n",
    "        # convert to ndarray\n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.values\n",
    "        # add bias column to x\n",
    "        if add_bias:\n",
    "            x = self.augmentation(x)\n",
    "        z = np.dot(x, self.w)\n",
    "        yhat = self.sigmoid(z).ravel()\n",
    "        if return_proba:\n",
    "            return yhat\n",
    "        else:\n",
    "            return (yhat >= threshold) * 1\n",
    "        \n",
    "    def augmentation(self, x):\n",
    "        # add bias column to x\n",
    "        n, m = x.shape\n",
    "        ones = np.ones((n, 1))\n",
    "        return np.hstack((ones, x))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # sigmoid function\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def cross_entropy_loss(self, y, yhat):\n",
    "        # calculate cross entropy loss : -y*log(yhat) - (1-y)*log(1-yhat)\n",
    "\n",
    "        loss = -y*np.log(yhat) - (1-y)*np.log(1-yhat)\n",
    "        return loss\n",
    "\n",
    "def split_data(data, target, ratio=0.8):\n",
    "    n = data.shape[0]\n",
    "    ind = np.arange(n)\n",
    "    split = int(n*ratio)\n",
    "    return data[:split], target[:split], data[split:], target[split:]\n",
    "\n",
    "def confusion_matrix(y, y_pred):\n",
    "    # binary classification confusion matrix\n",
    "    ind_t = np.nonzero(y==1)[0] # positive label index\n",
    "    ind_f = np.nonzero(y==0)[0] # negative label index\n",
    "    tp = (y_pred[ind_t]==1).sum() # true positive\n",
    "    fn = (y_pred[ind_t]==0).sum() # false negative\n",
    "    tn = (y_pred[ind_f]==0).sum() # true negative\n",
    "    fp = (y_pred[ind_f]==1).sum() # false negative\n",
    "    return tp,fn,tn,fp\n",
    "\n",
    "def misclassification_rate(y, y_pred):\n",
    "    return (y!=y_pred).sum() / len(y)\n",
    "\n",
    "def precision(y, y_pred):\n",
    "    # precision = TP / (TP+FP)  \n",
    "    tp,fn,tn,fp = confusion_matrix(y, y_pred)\n",
    "    return tp / (tp+fp)\n",
    "\n",
    "def recall(y, y_pred):\n",
    "    # recall = TP / (TP+FN)\n",
    "    tp,fn,tn,fp = confusion_matrix(y, y_pred)\n",
    "    return tp / (tp+fn)\n",
    "\n",
    "def f1_score(y, y_pred):\n",
    "    # calculate f1 score = 2* (precision*recall) / (precision+recall)\n",
    "    p = precision(y, y_pred)\n",
    "    r = recall(y, y_pred)\n",
    "    return 2 * (p*r) / (p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:55:05.565997Z",
     "start_time": "2021-12-09T04:55:05.548037Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-15T01:14:30.691195Z",
     "iopub.status.busy": "2020-10-15T01:14:30.691195Z",
     "iopub.status.idle": "2020-10-15T01:14:30.783474Z",
     "shell.execute_reply": "2020-10-15T01:14:30.781480Z",
     "shell.execute_reply.started": "2020-10-15T01:14:30.691195Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "iris = load_breast_cancer()\n",
    "data = iris.data # (569, 30)\n",
    "target = iris.target # (569,), label:0,1\n",
    "\n",
    "# split data\n",
    "xtrain, ytrain, xtest, ytest = split_data(data, target, 0.8)\n",
    "\n",
    "# standardization\n",
    "std = StandardScaler().fit(xtrain)\n",
    "std_xtrain = std.transform(xtrain)\n",
    "std_xtest = std.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-09T04:55:12.101709Z",
     "start_time": "2021-12-09T04:55:12.063811Z"
    },
    "execution": {
     "iopub.execute_input": "2020-10-15T01:14:46.413522Z",
     "iopub.status.busy": "2020-10-15T01:14:46.413522Z",
     "iopub.status.idle": "2020-10-15T01:15:01.340211Z",
     "shell.execute_reply": "2020-10-15T01:15:01.339247Z",
     "shell.execute_reply.started": "2020-10-15T01:14:46.413522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of testing set = 0.9772727272727273\n",
      "misclassification rate = 0.03508771929824561\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "batch_size = len(std_xtrain) # using full batch\n",
    "my_lr = logistic_regression(learning_rate=0.01, max_iter=100, batch_size=batch_size)\n",
    "my_lr = my_lr.fit(std_xtrain, ytrain)\n",
    "\n",
    "# predict testing set using different metrics\n",
    "yhat = my_lr.predict(std_xtest)\n",
    "print('f1 score of testing set =', f1_score(ytest, yhat))\n",
    "print('misclassification rate =', misclassification_rate(ytest, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
